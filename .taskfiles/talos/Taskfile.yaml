---
# yaml-language-server: $schema=https://taskfile.dev/schema.json
version: '3'

vars:
  TALHELPER_CONFIG_FILE: '{{.CLUSTER_DIR}}/talos/talconfig.yaml'
  TALHELPER_SECRET_FILE: '{{.CLUSTER_DIR}}/talos/talsecret.sops.yaml'
  TALHELPER_CLUSTER_DIR: '{{.CLUSTER_DIR}}/talos/clusterconfig'

tasks:
  gen-config:
    desc: Generate the config files for a Talos cluster [CLUSTER=main]
    cmds:
      - |
        if [ ! -f '{{.TALHELPER_SECRET_FILE}}' ]; then
            talhelper gensecret > {{.TALHELPER_SECRET_FILE}}
            sops --encrypt --in-place {{.TALHELPER_SECRET_FILE}}
        fi
      - talhelper genconfig --config-file {{.TALHELPER_CONFIG_FILE}} --secret-file {{.TALHELPER_SECRET_FILE}} --out-dir {{.TALHELPER_CLUSTER_DIR}}
      # - talosctl config merge {{.TALHELPER_CLUSTER_DIR}}/talosconfig # TODO: force overwrite?
    requires:
      vars: [CLUSTER]
    preconditions:
      - test -f {{.TALHELPER_CONFIG_FILE}}
      - which talhelper sops talosctl

  apply-all:
    desc: Apply Talos config to all nodes in the cluster [CLUSTER=main] [FLAGS='--insecure']
    cmds:
      - talhelper gencommand apply --config-file {{.TALHELPER_CONFIG_FILE}} --out-dir {{.TALHELPER_CLUSTER_DIR}} --extra-flags={{.FLAGS}} | bash
    requires:
      vars: [CLUSTER]
    vars:
      FLAGS: '{{.FLAGS | default "--insecure" }}'
    preconditions:
      - test -f {{.ROOT_DIR}}/talosconfig
      - which talhelper

  apply-node:
    desc: Apply Talos config to a node [CLUSTER=main] [HOSTNAME=required]
    cmds:
      - task: down
      - talosctl --context {{.CLUSTER}} --nodes {{.HOSTNAME}} apply-config --file {{.CLUSTER_DIR}}/talos/clusterconfig/{{.CLUSTER}}-{{.HOSTNAME}}
      - talosctl --context {{.CLUSTER}} --nodes {{.HOSTNAME}} health --wait-timeout=10m --server=false
      - task: up
    requires:
      vars: [CLUSTER, HOSTNAME]
    preconditions:
      - talosctl --context {{.CLUSTER}} --nodes {{.HOSTNAME}} get machineconfig
      - test -f {{.CLUSTER_DIR}}/talos/clusterconfig/{{.CLUSTER}}-{{.HOSTNAME}}
      - test -f {{.ROOT_DIR}}/talosconfig
      - which talosctl

  upgrade-node:
    desc: Upgrade Talos on a single node [CLUSTER=main] [HOSTNAME=required]
    dotenv:
      - '{{.CLUSTER_DIR}}/cluster.env'
    cmds:
      - task: down
      - talosctl --context {{.CLUSTER}} --nodes {{.HOSTNAME}} upgrade --image="factory.talos.dev/installer/{{.SCHEMATIC_ID}}:$TALOS_VERSION" --timeout=10m
      - talosctl --context {{.CLUSTER}} --nodes {{.HOSTNAME}} health --wait-timeout=10m --server=false
      - task: up
    vars:
      SCHEMATIC_ID:
        sh: kubectl --context {{.CLUSTER}} get node {{.HOSTNAME}} --output=jsonpath='{.metadata.annotations.extensions\.talos\.dev/schematic}'
    requires:
      vars: [CLUSTER, HOSTNAME]
    preconditions:
      - curl -fsSL -o /dev/null --fail https://github.com/siderolabs/talos/releases/tag/$TALOS_VERSION
      - talosctl --context {{.CLUSTER}} --nodes {{.HOSTNAME}} get machineconfig
      - talosctl --context {{.CLUSTER}} config info
      - test -f {{.CLUSTER_DIR}}/cluster.env
      - test -f {{.ROOT_DIR}}/talosconfig
      - which kubectl talosctl

  upgrade-k8s:
    desc: Upgrade Kubernetes across the whole cluster [CLUSTER=main]
    dotenv:
      - '{{.CLUSTER_DIR}}/cluster.env'
    cmds:
      - task: down
      - talosctl --context {{.CLUSTER}} --nodes {{.CONTROLLER}} upgrade-k8s --to $KUBERNETES_VERSION
      - task: up
    vars:
      CONTROLLER:
        sh: talosctl --context {{.CLUSTER}} config info --output json | jq --raw-output '.endpoints[]' | shuf -n 1
    requires:
      vars: [CLUSTER]
    preconditions:
      - curl -fsSL -o /dev/null --fail https://github.com/siderolabs/kubelet/releases/tag/$KUBERNETES_VERSION
      - talosctl --context {{.CLUSTER}} --nodes {{.CONTROLLER}} get machineconfig
      - talosctl --context {{.CLUSTER}} config info
      - test -f {{.CLUSTER_DIR}}/cluster.env
      - test -f {{.ROOT_DIR}}/talosconfig
      - which talosctl jq

  reboot-node:
    desc: Reboot Talos on a single node [CLUSTER=main] [HOSTNAME=required]
    cmds:
      - task: down
      - talosctl --context {{.CLUSTER}} --nodes {{.HOSTNAME}} reboot
      - talosctl --context {{.CLUSTER}} --nodes {{.HOSTNAME}} health --wait-timeout=10m --server=false
      - task: up
    requires:
      vars: [CLUSTER, HOSTNAME]
    preconditions:
      - talosctl --context {{.CLUSTER}} --nodes {{.HOSTNAME}} get machineconfig
      - talosctl --context {{.CLUSTER}} config info
      - test -f {{.CLUSTER_DIR}}/cluster.env
      - test -f {{.ROOT_DIR}}/talosconfig
      - which talosctl

  reset-cluster:
    desc: Resets nodes back to maintenance mode [CLUSTER=main]
    prompt: Reset the Talos cluster '{{.CLUSTER}}' ... continue?
    dir: '{{.CLUSTER_DIR}}/talos'
    cmd: talhelper gencommand reset --extra-flags '--reboot --system-labels-to-wipe STATE --system-labels-to-wipe EPHEMERAL --graceful=false --wait=false' | bash
    requires:
      vars: [CLUSTER]
    preconditions:
      - test -f '{{.CLUSTER_DIR}}/talos/talconfig.yaml'
      - which talhelper

  kubeconfig:
    desc: Generate the kubeconfig for a Talos cluster [CLUSTER=main]
    cmd: talosctl --context {{.CLUSTER}} kubeconfig --nodes {{.TALOS_CONTROLLER}} --merge --force --force-context-name {{.CLUSTER}} {{.ROOT_DIR}}
    vars:
      TALOS_CONTROLLER:
        sh: talosctl --context {{.CLUSTER}} config info --output json | jq --raw-output '.endpoints[]' | shuf -n 1
    requires:
      vars: [CLUSTER]
    preconditions:
      - talosctl --context {{.CLUSTER}} config info
      - test -f {{.ROOT_DIR}}/talosconfig
      - which talosctl jq

  down:
    internal: true
    cmds:
      - flux --namespace flux-system suspend kustomization --all
      - until kubectl --context {{.CLUSTER}} wait cephcluster --for=jsonpath=.status.ceph.health=HEALTH_OK --timeout=10m --all --all-namespaces &>/dev/null; do sleep 5; done
      - until kubectl --context {{.CLUSTER}} wait jobs --all --all-namespaces --for=condition=complete --timeout=5m &>/dev/null; do sleep 5; done
    preconditions:
      - which flux kubectl

  up:
    internal: true
    cmds:
      - flux --namespace flux-system resume kustomization --all
      - until kubectl --context {{.CLUSTER}} wait cephcluster --for=jsonpath=.status.ceph.health=HEALTH_OK --timeout=10m --all --all-namespaces &>/dev/null; do sleep 5; done
      - until kubectl --context {{.CLUSTER}} wait jobs --all --all-namespaces --for=condition=complete --timeout=5m &>/dev/null; do sleep 5; done
    preconditions:
      - which flux kubectl
