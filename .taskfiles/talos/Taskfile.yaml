---
# yaml-language-server: $schema=https://taskfile.dev/schema.json
version: '3'

vars:
  TALOS_SECRETS_FILE: '{{.BOOTSTRAP_DIR}}/talos/talos_secrets.sops.yaml'

tasks:
  gen-secret:
    desc: Generate a new Talos secret file
    prompt: Generate a new Talos secret file ... this will invalidate previously created talosconfig and machineconfig files ... continue?
    cmds:
      - talosctl gen secrets --force -o {{.TALOS_SECRETS_FILE}}
      - sops encrypt --in-place --input-type yaml --output-type yaml {{.TALOS_SECRETS_FILE}}
      - task: talosconfig
        vars:
          TALOS_NODES: '{{.TALOS_NODES}}'
    preconditions:
      - test -f {{.SOPS_AGE_KEY_FILE}}
      - which talosctl sops

  talosconfig:
    desc: Generate a new talosconfig file from the current Talos secrets file
    cmds:
      - sops exec-file --input-type yaml --output-type yaml {{.TALOS_SECRETS_FILE}} "talosctl gen config --force --with-secrets {} --output-types talosconfig -o {{.CLUSTER_DIR}}/talosconfig main https://not-used:6443"
      - cmd: echo 'Generated new talosconfig file, endpoints and nodes arrays will need to be manually populated'
        silent: true
    preconditions:
      - test -f {{.TALOS_SECRETS_FILE}}
      - which talosctl

  create-config:
    desc: Generate a new Talos configuration file [IP=required] [ENDPOINT=required] [NODE_TYPE=controlplane]
    cmds:
      - >
        sops exec-file --input-type yaml --output-type yaml {{.TALOS_SECRETS_FILE}}
        "talosctl gen config --with-secrets {} --config-patch @{{.NODE_PATCH_FILE}}
        --config-patch-{{if eq .NODE_TYPE "controlplane"}}control-plane{{else}}worker{{end}} @{{.NODE_TYPE_PATCH_FILE}}
        --output-types {{.NODE_TYPE}} --output {{.FILENAME}} --with-docs=false --with-examples=false --force main {{.ENDPOINT}}"
      - sops encrypt --in-place --input-type yaml --output-type yaml {{.FILENAME}}
    vars:
      FILENAME: '{{.BOOTSTRAP_DIR}}/talos/{{.IP}}.sops.yaml.j2'
      NODE_TYPE: '{{.NODE_TYPE | default "controlplane"}}'
      NODE_TYPE_PATCH_FILE: '{{.BOOTSTRAP_DIR}}/talos/patches/{{.NODE_TYPE}}.patch'
      NODE_PATCH_FILE: '{{.BOOTSTRAP_DIR}}/talos/patches/nodes/{{.IP}}.patch'
    requires:
      vars: [IP, ENDPOINT]
    preconditions:
      - test -f {{.TALOS_SECRETS_FILE}}
      - test -f {{.CLUSTER_DIR}}/talosconfig
      - test -f {{.NODE_TYPE_PATCH_FILE}}
      - test -f {{.NODE_PATCH_FILE}}
      - which sops talosctl

  apply-node:
    desc: Apply Talos config to a node [IP=required] [MODE=auto]
    cmds:
      - task: down
      - sops exec-file --input-type yaml --output-type yaml {{.BOOTSTRAP_DIR}}/talos/{{.IP}}.sops.yaml.j2 "minijinja-cli {}" | talosctl --nodes {{.IP}} apply-config --mode={{.MODE}} --file /dev/stdin
      - talosctl --nodes {{.IP}} health
      - task: up
    vars:
      MODE: '{{.MODE | default "auto"}}'
    requires:
      vars: [IP]
    preconditions:
      - talosctl --nodes {{.IP}} get machineconfig
      - test -f {{.BOOTSTRAP_DIR}}/.cluster.env
      - test -f {{.BOOTSTRAP_DIR}}/talos/{{.IP}}.sops.yaml.j2
      - test -f {{.CLUSTER_DIR}}/talosconfig
      - which minijinja-cli sops talosctl

  upgrade-node:
    desc: Upgrade Talos on a single node [IP=required]
    cmds:
      - task: down
      - talosctl --nodes {{.IP}} upgrade --image="{{.TALOS_IMAGE}}" --timeout=10m
      - talosctl --nodes {{.IP}} health
      - task: up
    vars:
      TALOS_IMAGE:
        sh: yq '.machine.install.image' {{.BOOTSTRAP_DIR}}/talos/{{.IP}}.sops.yaml.j2 | minijinja-cli
    requires:
      vars: [IP]
    preconditions:
      - talosctl --nodes {{.IP}} get machineconfig
      - talosctl config info
      - test -f {{.BOOTSTRAP_DIR}}/.cluster.env
      - test -f {{.CLUSTER_DIR}}/talosconfig
      - which minijinja-cli talosctl yq

  upgrade-k8s:
    desc: Update Kubernetes across the whole cluster
    cmds:
      - task: down
      - talosctl --nodes {{.TALOS_CONTROLLER}} upgrade-k8s --to $KUBERNETES_VERSION
      - task: up
    vars:
      TALOS_CONTROLLER:
        sh: talosctl config info --output json | jq --raw-output '.endpoints[]' | shuf -n 1
    preconditions:
      - curl -fsSL -o /dev/null --fail https://github.com/siderolabs/kubelet/releases/tag/$KUBERNETES_VERSION
      - talosctl --nodes {{.TALOS_CONTROLLER}} get machineconfig
      - talosctl config info
      - test -f {{.BOOTSTRAP_DIR}}/.cluster.env
      - test -f {{.CLUSTER_DIR}}/talosconfig
      - which jq talosctl

  reboot-node:
    desc: Reboot Talos on a single node [IP=required] [MODE=default]
    cmds:
      - task: down
      - talosctl --nodes {{.IP}} reboot --mode={{.MODE}}
      - talosctl --nodes {{.IP}} health
      - task: up
    vars:
      MODE: '{{.MODE | default "default"}}'
    requires:
      vars: [IP]
    preconditions:
      - talosctl --nodes {{.IP}} get machineconfig
      - talosctl config info
      - test -f {{.BOOTSTRAP_DIR}}/.cluster.env
      - test -f {{.CLUSTER_DIR}}/talosconfig
      - which talosctl

  shutdown-cluster:
    desc: Shutdown Talos across the whole cluster
    prompt: Shutdown the Talos cluster ... continue?
    cmd: talosctl shutdown --nodes {{.NODES}} --force
    vars:
      NODES:
        sh: talosctl config info --output json | jq --join-output '[.nodes[]] | join(",")'
    preconditions:
      - talosctl --nodes {{.NODES}} get machineconfig
      - talosctl config info
      - test -f {{.CLUSTER_DIR}}/.cluster.env
      - which jq talosctl

  reset-node:
    desc: Reset Talos on a single node [IP=required] [REBOOT=true]
    prompt: Reset Talos node '{{.IP}}' ... continue?
    cmd: talosctl reset --nodes {{.IP}} --graceful=false --reboot={{.REBOOT}}
    vars:
      REBOOT: '{{.REBOOT | default "true"}}'
    requires:
      vars: [IP]
    preconditions:
      - talosctl --nodes {{.IP}} get machineconfig
      - talosctl config info
      - test -f {{.CLUSTER_DIR}}/talosconfig
      - which talosctl

  reset-cluster:
    desc: Reset Talos across the whole cluster [REBOOT=true]
    prompt: Reset the Talos cluster ... This will require generating new secrets to create a new cluster ... continue?
    cmd: talosctl reset --nodes {{.NODES}} --graceful=false --reboot={{.REBOOT}}
    vars:
      NODES:
        sh: talosctl config info --output json | jq --join-output '[.nodes[]] | join(",")'
      REBOOT: '{{.REBOOT | default "true"}}'
    preconditions:
      - talosctl config info
      - test -f {{.CLUSTER_DIR}}/talosconfig
      - which jq talosctl

  kubeconfig:
    desc: Generate the kubeconfig for a Talos cluster
    cmd: talosctl kubeconfig --nodes {{.TALOS_CONTROLLER}} --force --force-context-name main {{.CLUSTER_DIR}}
    vars:
      TALOS_CONTROLLER:
        sh: talosctl config info --output json | jq --raw-output '.endpoints[]' | shuf -n 1
    preconditions:
      - talosctl config info
      - test -f {{.CLUSTER_DIR}}/talosconfig
      - which jq talosctl

  apply-cluster:
    desc: Apply Talos config across the whole cluster [MODE=auto]
    cmds:
      - for: { var: NODES }
        task: apply-node
        vars:
          IP: '{{.ITEM}}'
          MODE: '{{.MODE}}'
    vars:
      MODE: '{{.MODE | default "auto"}}'
      NODES:
        sh: talosctl config info --output json | jq --join-output '[.nodes[]] | join(" ")'
    preconditions:
      - which jq talosctl

  upgrade-cluster:
    desc: Upgrade Talos across the whole cluster
    cmds:
      - for: { var: NODES }
        task: upgrade-node
        vars:
          IP: '{{.ITEM}}'
    vars:
      NODES:
        sh: talosctl config info --output json | jq --join-output '[.nodes[]] | join(" ")'
    preconditions:
      - which jq talosctl

  reboot-cluster:
    desc: Reboot Talos across the whole cluster [MODE=default]
    cmds:
      - for: { var: NODES }
        task: reboot-node
        vars:
          IP: '{{.ITEM}}'
          MODE: '{{.MODE}}'
    vars:
      NODES:
        sh: talosctl config info --output json | jq --join-output '[.nodes[]] | join(" ")'
    preconditions:
      - which jq talosctl

  down:
    internal: true
    cmds:
      - until kubectl wait cephcluster --for=jsonpath=.status.ceph.health=HEALTH_OK --timeout=10m --all --all-namespaces &>/dev/null; do sleep 5; done
      - until kubectl wait jobs --all --all-namespaces --for=condition=complete --timeout=5m &>/dev/null; do sleep 5; done
    preconditions:
      - which kubectl

  up:
    internal: true
    cmds:
      - until kubectl wait cephcluster --for=jsonpath=.status.ceph.health=HEALTH_OK --timeout=10m --all --all-namespaces &>/dev/null; do sleep 5; done
      - until kubectl wait jobs --all --all-namespaces --for=condition=complete --timeout=5m &>/dev/null; do sleep 5; done
    preconditions:
      - which kubectl
