---
# yaml-language-server: $schema=https://taskfile.dev/schema.json
version: '3'

vars:
  BOOTSTRAP_RESOURCES_DIR: '{{.ROOT_DIR}}/.taskfiles/bootstrap/resources'

tasks:
  talos:
    desc: Bootstrap Talos [CLUSTER=main]
    cmds:
      - until talosctl --context {{.CLUSTER}} --nodes {{.TALOS_CONTROLLER}} bootstrap; do sleep 5; done
      - talosctl --context {{.CLUSTER}} kubeconfig --nodes {{.TALOS_CONTROLLER}} --merge --force --force-context-name {{.CLUSTER}} {{.ROOT_DIR}}
    vars:
      TALOS_CONTROLLER:
        sh: talosctl --context {{.CLUSTER}} config info --output json | jq --raw-output '.endpoints[]' | shuf -n 1
    requires:
      vars: [CLUSTER]
    preconditions:
      - talosctl --context {{.CLUSTER}} config info
      - test -f {{.ROOT_DIR}}/talosconfig
      - which talosctl jq

  # NOTE: Nodes must all be part of the Ceph cluster and Ceph disks must share the same disk model
  rook:
    desc: Bootstrap Rook-Ceph [CLUSTER=main] [MODEL=required]
    cmds:
      - minijinja-cli {{.BOOTSTRAP_RESOURCES_DIR}}/wipe-rook-data.yaml.j2 | kubectl --context {{.CLUSTER}} apply --server-side --filename -
      - until kubectl --context {{.CLUSTER}} --namespace default get job/wipe-rook-data &>/dev/null; do sleep 5; done
      - kubectl --context {{.CLUSTER}} --namespace default wait job/wipe-rook-data --for=condition=complete --timeout=5m
      - kubectl --context {{.CLUSTER}} --namespace default logs job/wipe-rook-data
      - kubectl --context {{.CLUSTER}} --namespace default delete job wipe-rook-data
      - minijinja-cli {{.BOOTSTRAP_RESOURCES_DIR}}/wipe-rook-disk.yaml.j2 | kubectl --context {{.CLUSTER}} apply --server-side --filename -
      - until kubectl --context {{.CLUSTER}} --namespace default get job/wipe-rook-disk &>/dev/null; do sleep 5; done
      - kubectl --context {{.CLUSTER}} --namespace default wait job/wipe-rook-disk --for=condition=complete --timeout=5m
      - kubectl --context {{.CLUSTER}} --namespace default logs job/wipe-rook-disk
      - kubectl --context {{.CLUSTER}} --namespace default delete job wipe-rook-disk
    env:
      NODES:
        sh: talosctl --context {{.CLUSTER}} config info --output json | jq --raw-output '.nodes | length'
    preconditions:
      - test -f {{.BOOTSTRAP_RESOURCES_DIR}}/wipe-rook-data.yaml.j2
      - test -f {{.BOOTSTRAP_RESOURCES_DIR}}/wipe-rook-disk.yaml.j2
      - which kubectl minijinja-cli talosctl jq

  apps:
    desc: Bootstrap Apps [CLUSTER=main]
    cmds:
      - until kubectl --context {{.CLUSTER}} wait nodes --for=condition=Ready=False --all --timeout=10m; do sleep 5; done
      - helmfile --quiet --file {{.CLUSTER_DIR}}/bootstrap/apps/helmfile.yaml apply --skip-diff-on-install --suppress-diff
      - until kubectl --context {{.CLUSTER}} wait nodes --for=condition=Ready --all --timeout=10m; do sleep 5; done
    requires:
      vars: [CLUSTER]
    preconditions:
      - talosctl --context {{.CLUSTER}} config info
      - test -f {{.CLUSTER_DIR}}/bootstrap/apps/helmfile.yaml
      - which helmfile kubectl talosctl

  flux:
    desc: Bootstrap Flux [CLUSTER=main]
    cmds:
      - kubectl --context {{.CLUSTER}} apply --server-side --kustomize {{.CLUSTER_DIR}}/bootstrap/apps
      - for: { var: TEMPLATES }
        cmd: op run --env-file {{.CLUSTER_DIR}}/bootstrap/bootstrap.env --no-masking -- minijinja-cli {{.ITEM}} | kubectl --context {{.CLUSTER}} apply --server-side --filename -
      - kubectl --context {{.CLUSTER}} apply --server-side --kustomize {{.CLUSTER_DIR}}/flux/config
    vars:
      TEMPLATES:
        sh: ls {{.CLUSTER_DIR}}/bootstrap/apps/*.j2
    env:
      VAULT: '{{if eq .CLUSTER "main"}}kubernetes{{else}}{{.CLUSTER}}{{end}}'
      FLUX_GITHUB_PUBLIC_KEYS:
        sh: curl -fsSl https://api.github.com/meta | jq --raw-output '"github.com "+.ssh_keys[]'
    requires:
      vars: [CLUSTER]
    preconditions:
      - op user get --me
      - which curl flux kubectl ls op jq
